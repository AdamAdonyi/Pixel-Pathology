# -*- coding: utf-8 -*-
"""XAI_UE_LIME_Share.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CGzzv41eec05eEn5iXieklY208lTiUGn

# 2024 WS ExplainableAI

ðŸ« X-Ray Vision: Unveiling What Models See in Chest Radiographs
Using LIME to peek into the black box of chest X-ray classification models
"""

!pip install lime

import kagglehub
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
import numpy as np
from lime import lime_image
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt
import os

import kagglehub

# Download latest version
path = kagglehub.dataset_download("pritpal2873/chest-x-ray-dataset-4-categories")

print("Path to dataset files:", path)

# Find actually location
import os

def find_dataset_location():
    # Common locations to check
    possible_locations = [
        '/content',
        '/root/.cache/kagglehub/datasets',
        '/kaggle/working'
    ]

    print("Searching for dataset...")
    for base_path in possible_locations:
        print(f"\nChecking in {base_path}")
        if os.path.exists(base_path):
            for root, dirs, files in os.walk(base_path):
                print(f"Found directory: {root}")
                if any(file.endswith('.jpg') or file.endswith('.png') for file in files):
                    print(f"Found images in: {root}")
                    return root
    return None


dataset_location = find_dataset_location()
print(f"\nDataset location: {dataset_location}")

# List the contents to reuse
if dataset_location:
    print("\nContents:")
    print(os.listdir(dataset_location))

import os

directory = '/root/.cache/kagglehub/datasets/pritpal2873/chest-x-ray-dataset-4-categories/versions/1/Chest X_Ray Dataset'

for file in os.listdir(directory):
  print(file)

"""# LIME"""

import os
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
from lime import lime_image
from lime.wrappers.scikit_image import SegmentationAlgorithm
from skimage.segmentation import mark_boundaries
from torchvision.models import convnext_tiny
import matplotlib.colors as mcolors
import matplotlib.cm as cm

# Memory-efficient model loading
def load_model(model_path):
    # Create the base model architecture
    model = convnext_tiny(weights=None)

    # Modify the classifier head for 4 classes
    num_classes = 4
    model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)

    # Load the state dict with weights_only
    state_dict = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)

    # Load state dict to the model
    model.load_state_dict(state_dict)
    model.eval()
    return model

# Prepare image preprocessing
def get_image_preprocessor():
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

# Create a prediction function for LIME with reduced memory usage
def create_prediction_fn(model, preprocessor):
    def predict_fn(images):
        # Process images in smaller batches
        batch_size = 10
        all_probs = []

        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]

            # Convert numpy images to tensors
            images_tensor = torch.stack([
                preprocessor(Image.fromarray((img * 255).astype(np.uint8)))
                for img in batch
            ])

            with torch.no_grad():
                outputs = model(images_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                all_probs.append(probabilities.numpy())

            # Clear GPU cache after each batch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return np.concatenate(all_probs, axis=0)

    return predict_fn

def load_dataset(dataset_dir, max_images_per_category=50, target_size=(224, 224)):
    # Use corrected category names
    categories = ['NORMAL', 'COVID19', 'PNEUMONIA', 'TURBERCULOSIS']
    images = []
    labels = []

    for label, category in enumerate(categories):
        category_path = os.path.join(dataset_dir, category)
        category_images = os.listdir(category_path)[:max_images_per_category]

        for img_file in category_images:
            img_path = os.path.join(category_path, img_file)
            try:
                # Open image and resize to target size
                img = Image.open(img_path).convert('RGB')
                img = img.resize(target_size, Image.LANCZOS)
                img = np.array(img) / 255.0  # Normalize to [0, 1]
                images.append(img)
                labels.append(label)
            except Exception as e:
                print(f"Error processing {img_path}: {e}")

    return np.array(images), np.array(labels)

def explain_image(model, image, prediction_fn, category_names):
    # Set up LIME explainer
    explainer = lime_image.LimeImageExplainer()
    segmenter = SegmentationAlgorithm('quickshift', kernel_size=1, max_dist=200, ratio=0.2)

    # Explain the image
    explanation = explainer.explain_instance(
        image,
        classifier_fn=prediction_fn,
        top_labels=len(category_names),
        hide_color=0,  # Gray out hidden regions
        num_samples=500,  # Reduced from 1000
        segmentation_fn=segmenter
    )

    # Get top predicted label
    top_label = explanation.top_labels[0]

    # Prepare the segmentation and local explanation
    segments = explanation.segments
    local_exp = explanation.local_exp[top_label]

    # Extract weights and normalize them
    weights = dict(local_exp)
    min_weight = min(weights.values())
    max_weight = max(weights.values())

    # Create a color-coded segmentation map
    colored_segments = np.zeros_like(image)
    unique_segments = np.unique(segments)

    # Create a color normalization
    norm = mcolors.Normalize(vmin=min_weight, vmax=max_weight)
    cmap = cm.coolwarm  # Red for positive, blue for negative importance

    # Color each segment based on its importance
    for segment in unique_segments:
        mask = (segments == segment)
        importance = weights.get(segment, 0)
        color = cmap(norm(importance))[:3]  # Convert to RGB
        colored_segments[mask] = color

    # Create visualization
    fig, axes = plt.subplots(1, 4, figsize=(20, 6))

    # Original image
    axes[0].imshow(image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # Full segmentation with color-coded importance
    axes[1].imshow(image * 0.5 + colored_segments * 0.5)  # Blend original and colored segments
    axes[1].set_title(f'Segmentation Importance\n{category_names[top_label]}')
    axes[1].axis('off')

    # Positive contributions
    temp, mask = explanation.get_image_and_mask(
        top_label,
        positive_only=True,
        num_features=5,
        hide_rest=True
    )
    axes[2].imshow(mark_boundaries(temp / 2 + 0.5, mask))
    axes[2].set_title(f'Positive Regions\n{category_names[top_label]}')
    axes[2].axis('off')

    # Colorbar and importance explanation
    axes[3].axis('off')  # Turn off axis for this subplot
    axes[3].set_title('Importance Scale')

    # Create a separate axes for the colorbar
    cax = axes[3].inset_axes([0.1, 0.1, 0.8, 0.8])
    sm = cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])

    # Add colorbar to the current axes
    fig.colorbar(sm, cax=cax, orientation='vertical',
                 label='Segment Importance')

    # Add text explanation
    cax.text(1.2, 0.5,
             'Red: Supports classification\n' +
             'White: Neutral\n' +
             'Blue: Against classification',
             verticalalignment='center',
             transform=cax.transAxes)

    plt.tight_layout()
    plt.show()

    # Print detailed segment information
    print(f"\nTop reasons for classifying as {category_names[top_label]}:")
    sorted_exp = sorted(local_exp, key=lambda x: abs(x[1]), reverse=True)
    for i, (feature, weight) in enumerate(sorted_exp[:10], 1):
        print(f"{i}. Segment {feature}: Importance = {weight:.4f}")

    return explanation

def main():
    # Set paths
    model_path = '/content/convnext_tiny_xray.pth'
    dataset_dir = '/root/.cache/kagglehub/datasets/pritpal2873/chest-x-ray-dataset-4-categories/versions/1/Chest X_Ray Dataset'

    # Categories with corrected spelling
    category_names = ['NORMAL', 'COVID19', 'PNEUMONIA', 'TURBERCULOSIS']

    # Load model and dataset
    model = load_model(model_path)

    # Load a smaller subset of images to prevent memory issues
    images, labels = load_dataset(dataset_dir, max_images_per_category=10)

    # Preprocessing
    preprocessor = get_image_preprocessor()
    prediction_fn = create_prediction_fn(model, preprocessor)

    # Select an example image from each category
    for label, category in enumerate(category_names):
        print(f"\nExplaining an example of {category}")
        # Find an image of this category
        category_images = images[labels == label]
        if len(category_images) > 0:
            example_image = category_images[0]

            # Explain the image
            explanation = explain_image(
                model,
                example_image,
                prediction_fn,
                category_names
            )

            # Clear memory after each explanation
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

if __name__ == '__main__':
    main()

